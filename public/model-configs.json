[
  {
    "id": "Llama-3-8B-Lexi-Uncensored-GGUF",
    "name": "Llama 3 8B Lexi Uncensored GGUF",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Llama-3-8B-Lexi-Uncensored-GGUF/resolve/main/Lexi-Llama-3-8B-Uncensored_Q5_K_M.gguf",
      "chat_batch_size": "16",
      "chat_ctx_size": "8096",
      "chat_name": "Lexi-Llama-3-8B-Uncensored_Q5_K_M",
      "description": "The default GaiaNet node config with a Lexi-Llama-3-8B-Uncensored model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_batch_size": "8192",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_name": "Nomic-embed-text-v1.5",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to directly answer the question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful assistant"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "Llama-3-8b_Rust_8k_PT_RPL",
    "name": "Llama 3 8b_Rust_8k_PT_RPL",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/Ak1104/combined_dataset_rust/resolve/main/unsloth.Q4_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "128",
      "chat_name": "llama-3-8b-rust",
      "description": "A Gaia node config with a finetuned Llama-3-8B-Instruct model and a Rust language knowledge base.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "You are an expert of the Rust language. When you provide Rust code in your answer, always make sure that the Rust code is correct. Answer questions accurately based on the text and code examples in the context.\n\nContext:\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/Ak1104/snapshot/resolve/main/rpl1_book.snapshot.tar.gz",
      "system_prompt": "You are an expert of the Rust language. Answer questions accurately. When you provide Rust code in your answer, always make sure that the Rust code is correct."
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 75
  },
  {
    "id": "codegemma-7b-it",
    "name": "codegemma 7b it",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/gemma-1.1-7b-it-GGUF/resolve/main/gemma-1.1-7b-it-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "16",
      "description": "gemma-1.1-7b-it",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "gemma-instruct",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "last-user-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "codestral-0.1-22b",
    "name": "codestral 0.1 22b",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Codestral-22B-v0.1-GGUF/resolve/main/Codestral-22B-v0.1-hf-Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "32",
      "description": "Codestral-0.1-22B",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "mistral-instruct",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are an expert software developer.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 22,
      "disk": 44,
      "gpu": "Optional"
    },
    "useCase": [
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "deepseek-2.5-236b",
    "name": "deepseek 2.5 236b",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/bartowski/DeepSeek-V2.5-GGUF/resolve/main/DeepSeek-V2.5-Q4_0/DeepSeek-V2.5-Q4_0.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "chat_name": "DeepSeek-V2.5-Q4_0",
      "description": "DeepSeek-V2.5",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "deepseek-chat-25",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are an experienced software developer.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 236,
      "disk": 472,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "deepseek-r1-distill-llama-70b",
    "name": "deepseek r1 distill llama 70b",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/DeepSeek-R1-Distill-Llama-70B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-70B-Q5_K_M.gguf",
      "chat_name": "DeepSeek-R1-Distill-Llama-70B-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "llama-3-chat",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 70,
      "disk": 140,
      "gpu": "Required"
    },
    "useCase": [],
    "performance": 90
  },
  {
    "id": "deepseek-r1-distill-llama-8b",
    "name": "deepseek r1 distill llama 8b",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf",
      "chat_name": "DeepSeek-R1-Distill-Llama-8B-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "llama-3-chat",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 75
  },
  {
    "id": "diplo",
    "name": "diplo",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "chat_batch_size": "128",
      "chat_ctx_size": "8096",
      "chat_name": "Llama-3-8B-Instruct",
      "description": "Llama-3.1-8B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_batch_size": "8192",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/diplo/resolve/main/diplo-v2.snapshot",
      "system_prompt": "You are Diplo, a famous DJ./n You are the guest of Gaia Launch Party on Sep 19th./n You are a co-creator and lead member of the electronic dancehall music project Major Lazer; a member of the supergroup LSD, with Labrinth and Sia; a member of electronic duo Jack Ü, with producer and DJ Skrillex; and a member of Silk City, with Mark Ronson. You founded the record company Mad Decent in 2006, as well as the non-profit organization Heaps Decent the following year. Your 2013 extended play (EP), Revolution, debuted at number 68 on the US Billboard 200. The EP's title track was later featured in a commercial for Hyundai and is featured on the WWE 2K16 soundtrack./n You also know a lot about Gaia and blockchain."
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "gemma-1.1-7b-it",
    "name": "gemma 1.1 7b it",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/gemma-1.1-7b-it-GGUF/resolve/main/gemma-1.1-7b-it-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "16",
      "description": "gemma-1.1-7b-it",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "gemma-instruct",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "last-user-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "gemma-2-27b-it",
    "name": "gemma 2 27b it",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/gemma-2-27b-it-GGUF/resolve/main/gemma-2-27b-it-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "16",
      "description": "gemma-2-27b-it",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "gemma-instruct",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant."
    },
    "requirements": {
      "memory": 27,
      "disk": 54,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 85
  },
  {
    "id": "gemma-2-9b-it",
    "name": "gemma 2 9b it",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "64",
      "chat_name": "gemma-2-9b-it-Q5_K_M",
      "description": "gemma-2-9b-it",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "gemma-instruct",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "last-user-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 9,
      "disk": 18,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "llama-2-7b",
    "name": "llama 2 7b",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "16",
      "description": "Llama-2-7b-chat",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-2-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "last-user-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "llama-3-70b-instruct",
    "name": "llama 3 70b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "16",
      "description": "Llama-3-70B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 70,
      "disk": 140,
      "gpu": "Required"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 90
  },
  {
    "id": "llama-3-8b-instruct",
    "name": "llama 3 8b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "16",
      "description": "Llama-3-8B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3-8b-instruct-262k",
    "name": "llama 3 8b instruct 262k",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/Llama-3-8B-Instruct-262k-GGUF/resolve/main/Llama-3-8B-Instruct-262k-Q5_K_M.gguf",
      "chat_ctx_size": "268288",
      "chat_batch_size": "16",
      "description": "Llama-3-8B-Instruct-262k",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3-8b-instruct_london",
    "name": "llama 3 8b instruct_london",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "16",
      "description": "Llama-3-8b-Instruct with London tour guide",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "rag_prompt": "You are a tour guide in London, UK. Use information in the following context to directly answer the question from a London visitor.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/london/resolve/main/london_768_nomic-embed-text-v1.5-f16.snapshot",
      "system_prompt": "You are a tour guide in London, UK. Please answer the question from a London visitor accurately.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3-8b-instruct_paris",
    "name": "llama 3 8b instruct_paris",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "16",
      "description": "Llama-3-8b-Instruct with Paris tour guide",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/paris/resolve/main/paris_768_nomic-embed-text-v1.5-f16.snapshot",
      "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3-Korean-Bllossom-8B",
    "name": "llama 3 Korean Bllossom 8B",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/llama-3-Korean-Bllossom-8B/resolve/main/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf",
      "chat_batch_size": "32",
      "chat_ctx_size": "4096",
      "description": "This GaiaNet node is a pure Korean LLM. You can chat it in Korean.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5-f16.gguf",
      "embedding_batch_size": "768",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "384",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "qdrant_limit": "3",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful and honest assistant. Always answer accurately, while being safe."
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "llama-3-groq-8b-tool",
    "name": "llama 3 groq 8b tool",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Llama-3-Groq-8B-Tool-Use-GGUF/resolve/main/Llama-3-Groq-8B-Tool-Use-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "16",
      "description": "Llama-3-Groq-8B-Tool",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "groq-llama3-tool",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 75
  },
  {
    "id": "llama-3.1-70b-instruct",
    "name": "llama 3.1 70b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/Meta-Llama-3.1-70B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "128",
      "chat_name": "Meta-Llama-3.1-70B-Instruct-Q4_K_M",
      "description": "Llama-3.1-70B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 70,
      "disk": 140,
      "gpu": "Required"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 90
  },
  {
    "id": "llama-3.1-8b-instruct",
    "name": "llama 3.1 8b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "8192",
      "chat_batch_size": "128",
      "chat_name": "Meta-Llama-3.1-8B-Instruct-Q5_K_M",
      "description": "Llama-3.1-8B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3.1-8b-instruct_basechain",
    "name": "llama 3.1 8b instruct_basechain",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "chat_batch_size": "128",
      "chat_ctx_size": "31072",
      "description": "Llama-3.1-8B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_batch_size": "8192",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/juntaoyuan/test-science/resolve/main/basechain.snapshot.tar.gz",
      "system_prompt": "You are a helpful assistant with expert knowledge on the Base blockchain. Whenever anyone mentions base or base chain, it always refers the Base blockchain, which is an Ethereum compatible (EVM) public blockchain developed by the Coinbase team led by Jesse Pollack.\n\n About Base: Base is a secure, low-cost, builder-friendly Ethereum L2 built to bring the next billion users onchain. Base is incubated within Coinbase and plans to progressively decentralize in the years ahead. We believe that decentralization is critical to creating an open, global cryptoeconomy that is accessible to everyone. The Base mainnet is launched on Aug 9th, 2023. Jesse Pollack is the creator of the Base chain. Previously, Jesse Pollack led all Consumer engineering at Coinbase.\n\n About Coinbase: Coinbase is an American publicly traded company that operates a cryptocurrency exchange platform. Coinbase is a distributed company; all employees operate via remote work. It is the largest cryptocurrency exchange in the United States in terms of trading volume. The company was founded in 2012 by Brian Armstrong and Fred Ehrsam."
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3.1-8b-instruct_chemistry",
    "name": "llama 3.1 8b instruct_chemistry",
    "configJson": {
      "address": "",
      "description": "Llama-3.1-8B-Instruct",
      "domain": "gaianet.network",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "chat_name": "llama-3.1-8b-chemistry",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "llama-3-chat",
      "reverse_prompt": "",
      "system_prompt": "You are a high school chemistry teacher. Always answer accurately and explain concepts. If the question is unrelated to chemistry or chemical elements, just answer 'I am here to help your learn chemistry, do you have a chemistry question?'",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "https://huggingface.co/datasets/gaianet/chemistry/resolve/main/chemistry-elements.snapshot.tar.gz",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "Answer the user request using chemistry concepts and facts in the following context. \n----------------\n"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Chemistry"
    ],
    "performance": 75
  },
  {
    "id": "llama-3.1-8b-instruct_pastor",
    "name": "llama 3.1 8b instruct_pastor",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "chat_batch_size": "128",
      "chat_ctx_size": "32768",
      "description": "Llama-3.1-8B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_batch_size": "8192",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "The following context provides an example on how to apply Bible teachings to real world challenges that are related to the user request. \n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/bible_bot/resolve/main/pastor-pairs.snapshot.tar.gz",
      "system_prompt": "You are a pastor helping your congregation to stay positive and feel better using Bible-based teachings. Do not answer any questions related to the following subjects. Just say: I do not know. Let's talk about something else\n * Hitler\n * Nazi\n * Communism\n * Black lives matter\n * LGBQT\n * Atheism\n * Islam\n * Slavery\n\n Do not give harmful or offensive information. Answer as concisely as possible for generic questions such as\n * Who are you?\n * How are you?\n * What's your name?\n * Do you know Bible?\n * Are you a therapist?\n * Do you have a degree?\n * How's the weather?\n\n Elaborate the answer when the user asks about a specific situation or challenge in life."
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3.1-8b-instruct_rustlang",
    "name": "llama 3.1 8b instruct_rustlang",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "chat_batch_size": "128",
      "chat_ctx_size": "32768",
      "description": "Llama-3.1-8B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_batch_size": "8192",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "Answer questions accurately based on the text and code examples in the context below.\n\nContext:\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/learn-rust/resolve/main/rust-books.snapshot.tar.gz",
      "system_prompt": "You are an AI coding instructor designed to assist and guide me as I learn to code. Your primary goal is to help me learn programming concepts, best practices, and problem-solving skills while writing code. Always assume I'm a beginner with limited programming knowledge.\n\nFollow these guidelines in all interactions:\n1. Explain concepts thoroughly but in simple terms, avoiding jargon when possible.\n2. When introducing new terms, provide clear definitions and examples.\n3. Break down complex problems into smaller, manageable steps.\n4. Encourage good coding practices and explain why they are important.\n5. Provide examples and analogies to illustrate programming concepts.\n6. Be patient and supportive, understanding that learning to code can be challenging.\n7. Offer praise for correct implementations and gentle corrections for mistakes.\n8. When correcting errors, explain why the error occurred and how to fix it.\n9. Suggest resources for further learning when appropriate.\n10. Encourage me to ask questions and seek clarification.\n11. Foster problem-solving skills by guiding me to find solutions rather than always providing direct answers.\n12. Adapt your teaching style to my pace and learning preferences.\n13. Provide code snippets to illustrate concepts, but always explain the code line by line.\n14. Use comments throughout the code to help document what is happening\n\nAddress the my questions thoroughly, keeping in mind the guidelines above. If the question is unclear or lacks context, ask me for clarification.\n\nReview the code and provide feedback. If there are errors or areas for improvement, explain them clearly and suggest corrections. If the code is correct, offer praise and explain why it's a good implementation.\n\nStructure your responses as follows:\n1. Format your response as markdown\n2. Answer my question\n3. Code review and feedback\n4. Suggestions for further learning or practice\n\nRemember, your goal is not just to help me write correct code, but to help me understand the underlying principles and develop my programming skills. Always strive to be clear, patient, and encouraging in your responses."
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3.1-8b-instruct_samsung-s24",
    "name": "llama 3.1 8b instruct_samsung s24",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
      "chat_batch_size": "128",
      "chat_ctx_size": "31072",
      "description": "Llama-3.1-8B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_batch_size": "8192",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/juntaoyuan/test-science/resolve/main/samsung.snapshot.tar.gz",
      "system_prompt": "You are a helpful assistant with expert knowledge on the Samsung Galaxy S24 smartphone. Whenever anyone mentions a device or a smartphone, it always refers the Samsung Galaxy S24 smartphone device. The Samsung Galaxy S24 is a series of Android-based smartphones designed, developed, manufactured, and marketed by Samsung Electronics as part of its flagship Galaxy S series. They collectively serve as the successor to the Galaxy S23 series. The phones were announced on January 17, 2024, at Galaxy Unpacked, alongside Galaxy AI, in San Jose, California. The phones were subsequently released on January 31, 2024. An all encompassing new mobile experience powered by both on-device and cloud-based AI solutions. Galaxy AI empowers the user experience through productivity, creativity, play, and more."
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 75
  },
  {
    "id": "llama-3.1-nemotron-70b-instruct",
    "name": "llama 3.1 nemotron 70b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config with a Llama 3.1 Nemotron 70B model.",
      "domain": "gaianet.network",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF/resolve/main/Llama-3.1-Nemotron-70B-Instruct-HF-Q5_K_M.gguf",
      "chat_name": "llama-3.1-nemotron-70b",
      "chat_ctx_size": "131072",
      "chat_batch_size": "128",
      "prompt_template": "llama-3-chat",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n"
    },
    "requirements": {
      "memory": 70,
      "disk": 140,
      "gpu": "Required"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 90
  },
  {
    "id": "llama-3.2-1b-instruct",
    "name": "llama 3.2 1b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config with a Llama 3.2 1B model.",
      "domain": "gaianet.network",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_M.gguf",
      "chat_name": "llama-3.2-1b",
      "chat_ctx_size": "8192",
      "chat_batch_size": "128",
      "prompt_template": "llama-3-chat",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n"
    },
    "requirements": {
      "memory": 1,
      "disk": 2,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "llama-3.2-1b-instruct_paris",
    "name": "llama 3.2 1b instruct_paris",
    "configJson": {
      "address": "",
      "description": "The Llama 3.2 1B model with a Paris tour guide.",
      "domain": "gaianet.network",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q5_K_M.gguf",
      "chat_name": "llama-3.2-1b",
      "chat_ctx_size": "8192",
      "chat_batch_size": "128",
      "prompt_template": "llama-3-chat",
      "reverse_prompt": "",
      "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "https://huggingface.co/datasets/gaianet/paris/resolve/main/paris_768_nomic-embed-text-v1.5-f16.snapshot",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n"
    },
    "requirements": {
      "memory": 1,
      "disk": 2,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "llama-3.2-3b-instruct",
    "name": "llama 3.2 3b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config with a Llama 3.2 3B model.",
      "domain": "gaianet.network",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
      "chat_name": "llama-3.2-3b",
      "chat_ctx_size": "8192",
      "chat_batch_size": "128",
      "prompt_template": "llama-3-chat",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n"
    },
    "requirements": {
      "memory": 3,
      "disk": 6,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "llama-3.2-3b-instruct_paris",
    "name": "llama 3.2 3b instruct_paris",
    "configJson": {
      "address": "",
      "description": "The Llama 3.2 3B model with a Paris tour guide.",
      "domain": "gaianet.network",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf",
      "chat_name": "llama-3.2-3b",
      "chat_ctx_size": "8192",
      "chat_batch_size": "128",
      "prompt_template": "llama-3-chat",
      "reverse_prompt": "",
      "system_prompt": "You are a tour guide in Paris, France. Please answer the question from a Paris visitor accurately.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "https://huggingface.co/datasets/gaianet/paris/resolve/main/paris_768_nomic-embed-text-v1.5-f16.snapshot",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n"
    },
    "requirements": {
      "memory": 3,
      "disk": 6,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "mathstral-7b",
    "name": "mathstral 7b",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/mathstral-7B-v0.1-GGUF/resolve/main/mathstral-7B-v0.1-Q5_K_M.gguf",
      "chat_batch_size": "16",
      "chat_ctx_size": "32000",
      "description": "mathstral-7b",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_batch_size": "8192",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "mistral-instruct",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "last-user-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant."
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "mistral-0.3-7b-instruct",
    "name": "mistral 0.3 7b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "32",
      "description": "GaiaNet node with a mistral 0.3 7b instruct model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "mistral-instruct",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "mistral-0.3-7b-instruct-tool-call",
    "name": "mistral 0.3 7b instruct tool call",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "32",
      "description": "GaiaNet node with a mistral 0.3 7b instruct model and tool call support.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "mistral-tool",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "openchat",
    "name": "openchat",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "64",
      "chat_name": "openchat-3.5-0106-Q5_K_M",
      "description": "openchat-3.5",
      "domain": "gaianet.network",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "openchat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "<|end_of_turn|>",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "last-user-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat"
    ],
    "performance": 70
  },
  {
    "id": "phi-3-medium-instruct-128k",
    "name": "phi 3 medium instruct 128k",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Phi-3-medium-128k-instruct-GGUF/resolve/main/Phi-3-medium-128k-instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a phi-3-medium 128k model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "phi-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "phi-3-medium-instruct-4k",
    "name": "phi 3 medium instruct 4k",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Phi-3-medium-4k-instruct-GGUF/resolve/main/Phi-3-medium-4k-instruct-Q5_K_M.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a phi-3-medium 4k model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "phi-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "phi-3-mini-instruct-128k",
    "name": "phi 3 mini instruct 128k",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Phi-3-mini-128k-instruct-GGUF/resolve/main/Phi-3-mini-128k-instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a phi-3-mini 128k model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "phi-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "phi-3-mini-instruct-128k_london",
    "name": "phi 3 mini instruct 128k_london",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Phi-3-mini-128k-instruct-GGUF/resolve/main/Phi-3-mini-128k-instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "16",
      "description": "Phi-3-mini-128k-instruct with London tour guide",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "phi-3-chat",
      "rag_prompt": "You are a tour guide in London, UK. Use information in the following context to answer questions from a London visitor.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/london/resolve/main/london_768_nomic-embed-text-v1.5-f16.snapshot",
      "system_prompt": "You are a tour guide in London, UK. Please answer questions from London visitors and tourists accurately and help them enjoy their stay in London.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "phi-3-mini-instruct-128k_paris",
    "name": "phi 3 mini instruct 128k_paris",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Phi-3-mini-128k-instruct-GGUF/resolve/main/Phi-3-mini-128k-instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "16",
      "description": "Phi-3-mini-128k-instruct with Paris tour guide",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "phi-3-chat",
      "rag_prompt": "You are a tour guide in Paris, France. Use information in the following context to answer questions from a Paris visitor.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/paris/resolve/main/paris_768_nomic-embed-text-v1.5-f16.snapshot",
      "system_prompt": "You are a tour guide in Paris, France. Please answer questions from Paris visitors and tourists accurately and help them enjoy their stay in Paris.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "phi-3-mini-instruct-4k",
    "name": "phi 3 mini instruct 4k",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a phi-3-mini model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "phi-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "phi-3-mini-instruct-4k_gaianet",
    "name": "phi 3 mini instruct 4k_gaianet",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "16",
      "description": "The default GaiaNet node config with a phi-3-mini model and a GaiaNet knowledge base.",
      "domain": "gaianet.network",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "phi-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question concisely.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/gaianet/resolve/main/gaianet-nomic-f16.snapshot",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately and concisely, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "phi-3.5-mini-instruct",
    "name": "phi 3.5 mini instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "128",
      "chat_name": "Phi-3.5-mini-instruct-Q5_K_M",
      "description": "Phi-3.5-mini-instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "phi-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen-1.5-0.5b-chat",
    "name": "qwen 1.5 0.5b chat",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen1.5-0.5B-Chat-GGUF/resolve/main/Qwen1.5-0.5B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a Qwen 1.5 0.5B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat"
    ],
    "performance": 70
  },
  {
    "id": "qwen-1.5-1.8b-chat",
    "name": "qwen 1.5 1.8b chat",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen1.5-1.8B-Chat-GGUF/resolve/main/Qwen1.5-1.8B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a Qwen 1.5 1.8B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 8,
      "disk": 16,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat"
    ],
    "performance": 75
  },
  {
    "id": "qwen-1.5-4b-chat",
    "name": "qwen 1.5 4b chat",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen1.5-4B-Chat-GGUF/resolve/main/Qwen1.5-4B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a Qwen 1.5 4B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 4,
      "disk": 8,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat"
    ],
    "performance": 70
  },
  {
    "id": "qwen-1.5-7b-chat",
    "name": "qwen 1.5 7b chat",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen1.5-7B-Chat-GGUF/resolve/main/Qwen1.5-7B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a Qwen 1.5 7B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-0.5b-instruct",
    "name": "qwen 2.5 coder 0.5b instruct",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 0.5B",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-0.5B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-0.5b",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "context_window": "1",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer, powered by qwencoder. You are happy to help answer any questions that the user has (usually they will be about coding).",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You will be asked to write or review code using programmig knowledge and best practices in the context below.\n\n# Context for knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-1.5b-instruct",
    "name": "qwen 2.5 coder 1.5b instruct",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 1.5B",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-1.5B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-1.5B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-1.5b",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer, powered by qwencoder. You are happy to help answer any questions that the user has (usually they will be about coding).",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You will be asked to write or review code using programmig knowledge and best practices in the context below.\n\n# Context for knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-14b-instruct",
    "name": "qwen 2.5 coder 14b instruct",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 14B",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-14b",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer, powered by qwencoder. You are happy to help answer any questions that the user has (usually they will be about coding).",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You will be asked to write or review code using programmig knowledge and best practices in the context below.\n\n# Context for knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 14,
      "disk": 28,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-14b-instruct_rustlang",
    "name": "qwen 2.5 coder 14b instruct_rustlang",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 14B with Rust knowledge",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-14B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-14b-rustlang",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer. You are happy to help answer any questions that the user has (usually they will be about coding). Please keep your response as concise as possible, and avoid being too verbose. Do not lie or make up facts. If a user messages you in a foreign language, please respond in that language. Format your response in markdown.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "https://huggingface.co/datasets/gaianet/learn-rust/resolve/main/rust-books.snapshot.tar.gz",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You are an experienced Rust developer. You will be asked to write or review code using Rust knowledge and best practices in the context below. You will be working with source code files in a typical Cargo project. The cargo.toml file contains the dependencies for external libraries and crates. The src/ directory contains main.rs or lib.rs or other Rust source code files. The project may also have SQL, text, HTML, Javascript, CSS and Python files. The user will give you the specific code files you need to work on.\n\n# Context for Rust knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 14,
      "disk": 28,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-32b-instruct",
    "name": "qwen 2.5 coder 32b instruct",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 32B",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-32b",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer, powered by qwencoder. You are happy to help answer any questions that the user has (usually they will be about coding).",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You will be asked to write or review code using programmig knowledge and best practices in the context below.\n\n# Context for knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 32,
      "disk": 64,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-32b-instruct_rustlang",
    "name": "qwen 2.5 coder 32b instruct_rustlang",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 32B with Rust knowledge",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-32b-rustlang",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer. You are happy to help answer any questions that the user has (usually they will be about coding). Please keep your response as concise as possible, and avoid being too verbose. Do not lie or make up facts. If a user messages you in a foreign language, please respond in that language. Format your response in markdown.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "https://huggingface.co/datasets/gaianet/learn-rust/resolve/main/rust-books.snapshot.tar.gz",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You are an experienced Rust developer. You will be asked to write or review code using Rust knowledge and best practices in the context below. You will be working with source code files in a typical Cargo project. The cargo.toml file contains the dependencies for external libraries and crates. The src/ directory contains main.rs or lib.rs or other Rust source code files. The project may also have SQL, text, HTML, Javascript, CSS and Python files. The user will give you the specific code files you need to work on.\n\n# Context for Rust knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 32,
      "disk": 64,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-3b-instruct",
    "name": "qwen 2.5 coder 3b instruct",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 3B",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-3B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-3B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-3b",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "context_window": "1",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer, powered by qwencoder. You are happy to help answer any questions that the user has (usually they will be about coding).",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You will be asked to write or review code using programmig knowledge and best practices in the context below.\n\n# Context for knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 3,
      "disk": 6,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-7b-instruct",
    "name": "qwen 2.5 coder 7b instruct",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 7B",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-7b",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer, powered by qwencoder. You are happy to help answer any questions that the user has (usually they will be about coding).",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You will be asked to write or review code using programmig knowledge and best practices in the context below.\n\n# Context for knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-7b-instruct_aelf",
    "name": "qwen 2.5 coder 7b instruct_aelf",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 7B with aelf knowledge",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-7b-aelf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer. You are happy to help answer any questions that the user has (usually they will be about coding). Please keep your response as concise as possible, and avoid being too verbose. Do not lie or make up facts. If a user messages you in a foreign language, please respond in that language. Format your response in markdown.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "https://huggingface.co/datasets/alabulei/aelf-test/blob/main/aelf.snapshot",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You are an experienced .Net C# developer for aelf blockchain smart contracts. You will be asked to write or review code using C# and aelf SDK knowledge and best practices in the context below. You will be working with source code files in a typical aelf project, which has three components.\n\nFirst, the smart contract RPC methods are defined in ProtoBuf documents in src/Protobuf/contract/*.proto files. The .proto files define how client apps or other smart contracts can call methods in your smart contract. The .proto files are compiled into C# source code by standard protoc tools available in VSCode. The RPC services defined in the .proto files have corresponding methods in the generated C# classes.\nSecond, you will need to inherit from the ContractState class, and create get and set methods for state data that needs to be stored on the aelf blockchain for this smart contract. The ContractState defines many data types that can be stored in the blockchain. The C# class for state management is in a src/*.cs file.\nThird, you need to inherit from the base class generated from the .proto file. You need to override service RPC methods from the base class to add application logic specific to this smart contract. For example, you might read and write the state data as part of the RPC service method. This smart contract class is also in a src/*.cs file.\n\nThe C# functions in the smart contract can be invoked remotely using aelf-command tools or aelf JavaScript SDK. The smart contract will functions will be executed by aelf blockchain nodes and have the state data recorded on the blockchain. The user will give you the specific code files you need to work on.\n\n# Context for aelf knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen-2.5-coder-7b-instruct_rustlang",
    "name": "qwen 2.5 coder 7b instruct_rustlang",
    "configJson": {
      "address": "",
      "description": "Qwen 2.5 Coder 7B with Rust knowledge",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "server_health_url": "https://pulse.gaianet.ai/node-health/0x",
      "server_info_url": "https://pulse.gaianet.ai/node-info/0x",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf",
      "chat_name": "qwen-2.5-coder-7b-rustlang",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are an intelligent programmer. You are happy to help answer any questions that the user has (usually they will be about coding). Please keep your response as concise as possible, and avoid being too verbose. Do not lie or make up facts. If a user messages you in a foreign language, please respond in that language. Format your response in markdown.",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "nomic-embed",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "https://huggingface.co/datasets/gaianet/learn-rust/resolve/main/rust-books.snapshot.tar.gz",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "You are an experienced Rust developer. You will be asked to write or review code using Rust knowledge and best practices in the context below. You will be working with source code files in a typical Cargo project. The cargo.toml file contains the dependencies for external libraries and crates. The src/ directory contains main.rs or lib.rs or other Rust source code files. The project may also have SQL, text, HTML, Javascript, CSS and Python files. The user will give you the specific code files you need to work on.\n\n# Context for Rust knowledge related to the user task\n\n"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "qwen2-0.5b-instruct",
    "name": "qwen2 0.5b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "32",
      "description": "This GaiaNet node config with a Qwen2 0.5B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2-0.5b-instruct_rustlang",
    "name": "qwen2 0.5b instruct_rustlang",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf",
      "chat_batch_size": "128",
      "chat_ctx_size": "131072",
      "description": "Qwen2-0.5B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_batch_size": "8192",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.2",
      "rag_policy": "system-message",
      "rag_prompt": "Answer questions accurately based on the text and code examples in the context below.\n\nContext:\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/Ak1104/snapshot/resolve/main/rbe.snapshot.tar.gz",
      "system_prompt": "You are an expert of the Rust language. When you provide Rust code in your answer, always make sure that the Rust code is correct."
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2-1.5b-instruct",
    "name": "qwen2 1.5b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen2-1.5B-Instruct-GGUF/resolve/main/Qwen2-1.5B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a Qwen2 1.5B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2-32b-instruct",
    "name": "qwen2 32b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen2-32B-Instruct-GGUF/resolve/main/Qwen2-32B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "32",
      "description": "This GaiaNet node config with a Qwen2 0.5B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 32,
      "disk": 64,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2-72b-instruct",
    "name": "qwen2 72b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen2-72B-Instruct-GGUF/resolve/main/Qwen2-72B-Instruct-Q4_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a Qwen2 72B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 72,
      "disk": 144,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2-7b-instruct",
    "name": "qwen2 7b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Qwen2-7B-Instruct-GGUF/resolve/main/Qwen2-7B-Instruct-Q5_K_M.gguf",
      "chat_ctx_size": "131072",
      "chat_batch_size": "16",
      "description": "This GaiaNet node config with a Qwen2 7B model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe.",
      "rag_policy": "system-message",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2.5-0.5b-instruct",
    "name": "qwen2.5 0.5b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf",
      "chat_name": "Qwen2.5-0.5B-Instruct-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2.5-1.5b-instruct",
    "name": "qwen2.5 1.5b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/Qwen2.5-1.5B-Instruct-Q5_K_M.gguf",
      "chat_name": "Qwen2.5-1.5B-Instruct-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2.5-14b-instruct",
    "name": "qwen2.5 14b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q5_K_M.gguf",
      "chat_name": "Qwen2.5-14B-Instruct-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 14,
      "disk": 28,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2.5-32b-instruct",
    "name": "qwen2.5 32b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-32B-Instruct-GGUF/resolve/main/Qwen2.5-32B-Instruct-Q5_K_M.gguf",
      "chat_name": "Qwen2.5-32B-Instruct-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 32,
      "disk": 64,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2.5-3b-instruct",
    "name": "qwen2.5 3b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-3B-Instruct-GGUF/resolve/main/Qwen2.5-3B-Instruct-Q5_K_M.gguf",
      "chat_name": "Qwen2.5-3B-Instruct-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 3,
      "disk": 6,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "qwen2.5-70b-instruct",
    "name": "qwen2.5 70b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-70B-Instruct-GGUF/resolve/main/Qwen2.5-70B-Instruct-Q5_K_M.gguf",
      "chat_name": "Qwen2.5-70B-Instruct-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 70,
      "disk": 140,
      "gpu": "Required"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 90
  },
  {
    "id": "qwen2.5-7b-instruct",
    "name": "qwen2.5 7b instruct",
    "configJson": {
      "address": "",
      "description": "The default GaiaNet node config",
      "domain": "gaia.domains",
      "llamaedge_port": "8080",
      "chat": "https://huggingface.co/gaianet/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q5_K_M.gguf",
      "chat_name": "Qwen2.5-7B-Instruct-Q5_K_M",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "prompt_template": "chatml",
      "reverse_prompt": "",
      "system_prompt": "You are a helpful assistant.",
      "context_window": "1",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_name": "Nomic-embed-text-v1.5",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "snapshot": "",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use information in the following context to answer the question.\n----------------\n"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 70
  },
  {
    "id": "reflection-llama-3.1-70b-instruct",
    "name": "reflection llama 3.1 70b instruct",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/Reflection-Llama-3.1-70B-GGUF/resolve/main/Reflection-Llama-3.1-70B-Q5_K_M.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "128",
      "chat_name": "Reflection-Llama-3.1-70B-Instruct-Q5_K_M",
      "description": "Reflection-Llama-3.1-70B-Instruct",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "llama-3-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 70,
      "disk": 140,
      "gpu": "Required"
    },
    "useCase": [
      "Instruction following"
    ],
    "performance": 90
  },
  {
    "id": "stablelm-2-zephyr-1.6b",
    "name": "stablelm 2 zephyr 1.6b",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf",
      "chat_ctx_size": "2048",
      "chat_batch_size": "16",
      "description": "GaiaNet node with a StableLM 2 Zephyr 1.6b model.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_collection_name": "default",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "stablelm-zephyr",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "rag_policy": "system-message",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant. Always answer accurately, while being safe."
    },
    "requirements": {
      "memory": 6,
      "disk": 12,
      "gpu": "Optional"
    },
    "useCase": [],
    "performance": 70
  },
  {
    "id": "vitalik.eth-7b-chat_vitalik-blog",
    "name": "vitalik.eth 7b chat_vitalik blog",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/vitalik.eth-7b/resolve/main/vitalik.eth-7b-q5_k_m.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "16",
      "description": "Llama 2 7B model finetuned with Vitalik Buterin's own writings, and supplemented with a knowledge collection from his blog posts.",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-ggml-model-f16.gguf",
      "embedding_ctx_size": "384",
      "llamaedge_port": "8080",
      "prompt_template": "llama-2-chat",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "https://huggingface.co/datasets/gaianet/vitalik.eth/resolve/main/vitalik.eth_384_all-minilm-l6-v2_f16.snapshot",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "3",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 7,
      "disk": 14,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat"
    ],
    "performance": 70
  },
  {
    "id": "yi-1.5-34b-chat-16k",
    "name": "yi 1.5 34b chat 16k",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Yi-1.5-34B-Chat-16K-GGUF/resolve/main/Yi-1.5-34B-Chat-16K-Q5_K_M.gguf",
      "chat_ctx_size": "16384",
      "chat_batch_size": "16",
      "description": "Yi-1.5-34B-Chat-16K",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "<|im_end|>",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 34,
      "disk": 68,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat"
    ],
    "performance": 70
  },
  {
    "id": "yi-1.5-9b-chat",
    "name": "yi 1.5 9b chat",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "16",
      "description": "Yi-1.5-9B-Chat",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 9,
      "disk": 18,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat"
    ],
    "performance": 70
  },
  {
    "id": "yi-coder-1.5b",
    "name": "yi coder 1.5b",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/gaianet/Yi-Coder-1.5B-Chat-GGUF/resolve/main/Yi-Coder-1.5B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "4096",
      "chat_batch_size": "16",
      "description": "Yi-Coder-1.5B",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "<|im_end|>",
      "snapshot": "",
      "system_prompt": "You are a helpful, respectful, and honest assistant.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "yi-coder-1.5b-chat",
    "name": "yi coder 1.5b chat",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/Yi-Coder-1.5B-Chat-GGUF/resolve/main/Yi-Coder-1.5B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "description": "Yi-Coder-9B-Chat",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "<|im_end|>",
      "snapshot": "",
      "system_prompt": "You are an expert software developer.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 5,
      "disk": 10,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "yi-coder-9b-chat",
    "name": "yi coder 9b chat",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/Yi-Coder-9B-Chat-GGUF/resolve/main/Yi-Coder-9B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "chat_name": "Yi-Coder-9B-Chat-Q5_K_M",
      "description": "Yi-Coder-9B-Chat",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "<|im_end|>",
      "snapshot": "",
      "system_prompt": "You are an expert software developer.",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5"
    },
    "requirements": {
      "memory": 9,
      "disk": 18,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat",
      "Code generation"
    ],
    "performance": 70
  },
  {
    "id": "yi-coder-9b-chat_rustlang",
    "name": "yi coder 9b chat_rustlang",
    "configJson": {
      "address": "",
      "chat": "https://huggingface.co/second-state/Yi-Coder-9B-Chat-GGUF/resolve/main/Yi-Coder-9B-Chat-Q5_K_M.gguf",
      "chat_ctx_size": "32768",
      "chat_batch_size": "128",
      "chat_name": "Yi-Coder-9B-Chat-Q5_K_M",
      "description": "Yi-Coder-9B-Chat",
      "domain": "gaia.domains",
      "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
      "embedding_ctx_size": "8192",
      "embedding_batch_size": "8192",
      "embedding_name": "nomic-embed-text-v1.5.f16",
      "llamaedge_port": "8080",
      "prompt_template": "chatml",
      "rag_prompt": "Use the following pieces of context to answer the user's question.\n----------------\n",
      "reverse_prompt": "<|im_end|>",
      "rag_policy": "system-message",
      "embedding_collection_name": "default",
      "qdrant_limit": "1",
      "qdrant_score_threshold": "0.5",
      "snapshot": "https://huggingface.co/datasets/gaianet/learn-rust/resolve/main/rust-books.snapshot.tar.gz",
      "system_prompt": "You are an AI coding instructor designed to assist and guide me as I learn to code in the Rust programming language. Your primary goal is to help me learn Rust programming concepts, best practices, and problem-solving skills while writing code. Always assume I'm a beginner with limited programming knowledge.\n\nFollow these guidelines in all interactions:\n1. Explain concepts thoroughly but in simple terms, avoiding jargon when possible.\n2. When introducing new terms, provide clear definitions and examples.\n3. Break down complex problems into smaller, manageable steps.\n4. Encourage good coding practices and explain why they are important.\n5. Provide examples and analogies to illustrate programming concepts.\n6. Be patient and supportive, understanding that learning to code can be challenging.\n7. Offer praise for correct implementations and gentle corrections for mistakes.\n8. When correcting errors, explain why the error occurred and how to fix it.\n9. Suggest resources for further learning when appropriate.\n10. Encourage me to ask questions and seek clarification.\n11. Foster problem-solving skills by guiding me to find solutions rather than always providing direct answers.\n12. Adapt your teaching style to my pace and learning preferences.\n13. Provide code snippets to illustrate concepts, but always explain the code line by line.\n14. Use comments throughout the code to help document what is happening\n\nAddress the my questions thoroughly, keeping in mind the guidelines above. If the question is unclear or lacks context, ask me for clarification.\n\nReview the code and provide feedback. If there are errors or areas for improvement, explain them clearly and suggest corrections. If the code is correct, offer praise and explain why it's a good implementation.\n\nStructure your responses as follows:\n1. Format your response as markdown\n2. Answer my question\n3. Code review and feedback\n4. Suggestions for further learning or practice\n\nRemember, your goal is not just to help me write correct code, but to help me understand the underlying principles and develop my programming skills. Always strive to be clear, patient, and encouraging in your responses."
    },
    "requirements": {
      "memory": 9,
      "disk": 18,
      "gpu": "Optional"
    },
    "useCase": [
      "Chat",
      "Code generation"
    ],
    "performance": 70
  }
]